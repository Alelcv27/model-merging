{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") or os.environ.get(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Set HF_TOKEN (or HUGGINGFACE_HUB_TOKEN) in your environment.\")\n",
    "\n",
    "login(\n",
    "  token = hf_token,\n",
    "  add_to_git_credential = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1096c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"./merged/Llama3-8B-Arcee-IST-Math\"  # folder to store the result in\n",
    "LORA_MERGE_CACHE = \"/tmp\"  # change if you want to keep these for some reason\n",
    "CONFIG_YML = \"./examples/llama/arcee_fusion.yml\"  # merge configuration file\n",
    "COPY_TOKENIZER = True  # you want a tokenizer? yeah, that's what i thought\n",
    "LAZY_UNPICKLE = False  # experimental low-memory model loader\n",
    "LOW_CPU_MEMORY = False  # enable if you somehow have more VRAM than RAM+swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a336f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "\n",
    "from mergekit.config import MergeConfiguration\n",
    "from mergekit.merge import MergeOptions, run_merge\n",
    "\n",
    "with open(CONFIG_YML, \"r\", encoding=\"utf-8\") as fp:\n",
    "    merge_config = MergeConfiguration.model_validate(yaml.safe_load(fp))\n",
    "\n",
    "run_merge(\n",
    "    merge_config,\n",
    "    out_path=OUTPUT_PATH,\n",
    "    options=MergeOptions(\n",
    "        lora_merge_cache=LORA_MERGE_CACHE,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        copy_tokenizer=COPY_TOKENIZER,\n",
    "        lazy_unpickle=LAZY_UNPICKLE,\n",
    "        low_cpu_memory=LOW_CPU_MEMORY,\n",
    "    ),\n",
    ")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92124523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "username = \"Alelcv27\"\n",
    "MODEL_NAME = \"Llama3-8B-Arcee-IST-Math\"\n",
    "\n",
    "api = HfApi(token = \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bae898",
   "metadata": {},
   "outputs": [],
   "source": [
    "api.create_repo(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path = f\"merged/{MODEL_NAME}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
